{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework part B: implementing a sample size estimator for interleaving experiments\n",
    "\n",
    "#### by Kim de Bie, Bram van den Heuvel and Kiki van Rongen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import texttable as tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simulate Rankings of Relevance for E and P\n",
    "\n",
    "In the first step we generate pairs of rankings, for the production P and experimental E, respectively. We assume a binary relevance. Documents can be distinct but they may also overlap. Further, we assume that the algorithms are used on mobiles, so we are interested only in rankings of length 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.16666666666666666: 26, 0.5: 17, 0.25: 17, 0.3333333333333333: 17, 0.125: 13, 0.375: 13, 0.08333333333333337: 13, 0.41666666666666674: 13, 0.08333333333333331: 13, 0.08333333333333326: 13, 0.33333333333333337: 5, 0.08333333333333334: 4, 0.625: 4, 0.2916666666666667: 4, 0.5833333333333334: 4, 0.25000000000000006: 4, 0.16666666666666663: 4, 0.41666666666666663: 4, 0.16666666666666669: 1, 0.45833333333333337: 1, 0.6666666666666666: 1})\n"
     ]
    }
   ],
   "source": [
    "class Ranking:\n",
    "\n",
    "    def __init__(self, id, ranker):\n",
    "        self.id = id\n",
    "        self.ERR = 0\n",
    "        self.ranker = ranker\n",
    "        self.ranking = [{'id': 1, 'relevance': 0}, {'id': 2, 'relevance': 0}, {'id': 3, 'relevance': 0}]\n",
    "\n",
    "    def set_ERR(self):\n",
    "        self.ERR = self.calculate_ERR()\n",
    "        \n",
    "    def calculate_ERR(self):\n",
    "\n",
    "        max_rl = 1\n",
    "\n",
    "        thetas = []\n",
    "        for idx, docu in enumerate(self.ranking):\n",
    "            rl = docu['relevance']\n",
    "            thetas.append((2**rl - 1) / (2**max_rl))\n",
    "            \n",
    "\n",
    "        # calculate ERR as sum of products\n",
    "        ERR = 0\n",
    "        for rank, doc in enumerate(self.ranking):\n",
    "            prod = 1\n",
    "            for i in range(rank):\n",
    "                prod *= (1-thetas[i])\n",
    "            ERR+= (prod * thetas[rank])/ (rank+1)\n",
    "            \n",
    "        return ERR\n",
    "    \n",
    "\n",
    "class Pair:\n",
    "    \n",
    "    def __init__(self, ranking_E, ranking_P):\n",
    "        self.rankings = [ranking_E, ranking_P]\n",
    "        self.delta_ERR = ranking_E.ERR - ranking_P.ERR\n",
    "\n",
    "\n",
    "def create_pairs():\n",
    "\n",
    "    \"\"\"\n",
    "    This function creates two ranked lists of documents for algorithm P and E.\n",
    "    Subsequently, it forms E-P pairs of possible rankings.\n",
    "    \"\"\"\n",
    "\n",
    "    ############### CREATE RANKED LIST FOR E ###############\n",
    "\n",
    "    rankings_E = []\n",
    "    rankings_P = []\n",
    "\n",
    "    # define all possible combinations of relevance labels\n",
    "    rl_permutations = [[0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [1, 0, 1], [0, 1, 1], [1, 1, 1]]\n",
    "\n",
    "    # assign id to the rankings\n",
    "    id = 0\n",
    "\n",
    "    # iterate over permutations of relevance labels\n",
    "    for rl_perm in rl_permutations:\n",
    "\n",
    "        # create ranking object and adjust its relevance labels\n",
    "        rank_E = Ranking(id, 'E')\n",
    "        for i, document_E in enumerate(rank_E.ranking):\n",
    "             document_E['relevance'] = rl_perm[i]\n",
    "\n",
    "        # calculate ERR\n",
    "        rank_E.set_ERR()\n",
    "\n",
    "        # store ranking list for E\n",
    "        rankings_E.append(rank_E)\n",
    "        id += 1\n",
    "\n",
    "    ############### CREATE RANKED LIST FOR P ###############\n",
    "\n",
    "    # define all possible id's for P\n",
    "    id_permutations = [[1, 5, 6], [2, 5, 6], [3, 5, 6], [4, 5, 6], \\\n",
    "                       [4, 1, 6], [4, 2, 6], [4, 3, 6], \\\n",
    "                       [4, 5, 1], [4, 5, 2], [4, 5, 3], \\\n",
    "                       [1, 2, 6], [1, 5, 2], [4, 1, 2], \\\n",
    "                       [1, 3, 6], [1, 5, 3], [4, 1, 3], \\\n",
    "                       [2, 3, 6], [2, 5, 3], [4, 2, 3], \\\n",
    "                       [2, 1, 6], [2, 5, 1], [4, 2, 1], \\\n",
    "                       [3, 1, 6], [3, 5, 1], [4, 3, 1], \\\n",
    "                       [3, 2, 6], [3, 5, 2], [4, 3, 2], \\\n",
    "                       [1, 2, 3], [1, 3, 2], [3, 1, 2], [3, 2, 1], [2, 1, 3], [2, 3, 1]]\n",
    "\n",
    "    # iterate over possible id's for P\n",
    "    for id_perm in id_permutations:\n",
    "\n",
    "        # iterate over permutations of relevance labels\n",
    "        for rl_perm in rl_permutations:\n",
    "\n",
    "            # create ranking object for P\n",
    "            rank_P = Ranking(id, 'P')\n",
    "\n",
    "            # adjust relevance labels & id numbers\n",
    "            for j, document_P in enumerate(rank_P.ranking):\n",
    "                document_P['relevance'] = rl_perm[j]\n",
    "                document_P['id'] = id_perm[j]\n",
    "\n",
    "            # calculate ERR\n",
    "            rank_P.set_ERR()\n",
    "\n",
    "            # store ranking list for P\n",
    "            rankings_P.append(rank_P)\n",
    "            id += 1\n",
    "\n",
    "    ############### FORM E-P PAIRS ###############\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for rank_E in rankings_E:\n",
    "\n",
    "        # store ids and relevance labels of E\n",
    "        ids_E = [1, 2, 3]\n",
    "        rl_E = [d.get('relevance') for d in rank_E.ranking]\n",
    "\n",
    "        for rank_P in rankings_P:\n",
    "\n",
    "            # keep track of errors (duplicates with non-matching rl's)\n",
    "            error = False\n",
    "\n",
    "            # store ids and relevance labels of P\n",
    "            ids_P = [d.get('id') for d in rank_P.ranking]\n",
    "            rl_P = [d.get('relevance') for d in rank_P.ranking]\n",
    "\n",
    "            # iterate over ids of P\n",
    "            for idx, id in enumerate(ids_P):\n",
    "\n",
    "                # check for duplicates with non-matching relevance labels\n",
    "                if (id in ids_E) & (rl_P[idx] != rl_E[idx]):\n",
    "                    error = True\n",
    "                    break\n",
    "\n",
    "            # create pair and add to list, if no error occurs\n",
    "            if not error:\n",
    "                pair = Pair(rank_E, rank_P)\n",
    "                if 0.05 < pair.delta_ERR < 0.95:\n",
    "                    pairs.append(pair)\n",
    "\n",
    "    return pairs\n",
    "\n",
    "pairs = create_pairs()\n",
    "counter = Counter()\n",
    "for i, p in enumerate(pairs):\n",
    "    counter[p.delta_ERR] += 1\n",
    "        \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate the 𝛥ERR\n",
    "Implement the aforementioned measure, ERR.\n",
    "\n",
    "For all P and E ranking pairs constructed above calculate the difference: 𝛥measure = measureE-measureP. Consider only those pairs for which E outperforms P and group them such that group 1 contains all pairs for which 0.05 < 𝛥measure ≤ 0.1, group 2 all pairs for which 0.1 < 𝛥measure ≤ 0.2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See previous step for loading pairs and calculation of (delta-)ERR.\n",
    "\n",
    "def create_groups(pairs):\n",
    "    \"\"\"\n",
    "    This function separates pairs into groups based on delta_ERR intervals.\n",
    "    It returns a list of sublists; each sublists contains an object of class Pair\n",
    "    \"\"\"\n",
    "\n",
    "    group1 = [p for p in pairs if (p.delta_ERR>=0.05 and p.delta_ERR<0.1)]\n",
    "    group2 = [p for p in pairs if (p.delta_ERR>=0.1 and p.delta_ERR<0.2)]\n",
    "    group3 = [p for p in pairs if (p.delta_ERR>=0.2 and p.delta_ERR<0.3)]\n",
    "    group4 = [p for p in pairs if (p.delta_ERR>=0.3 and p.delta_ERR<0.4)]\n",
    "    group5 = [p for p in pairs if (p.delta_ERR>=0.4 and p.delta_ERR<0.5)]\n",
    "    group6 = [p for p in pairs if (p.delta_ERR>=0.5 and p.delta_ERR<0.6)]\n",
    "    group7 = [p for p in pairs if (p.delta_ERR>=0.6 and p.delta_ERR<0.7)]\n",
    "    group8 = [p for p in pairs if (p.delta_ERR>=0.7 and p.delta_ERR<0.8)]\n",
    "    group9 = [p for p in pairs if (p.delta_ERR>=0.8 and p.delta_ERR<0.9)]\n",
    "    group10 = [p for p in pairs if (p.delta_ERR>=0.9 and p.delta_ERR<=0.95)]\n",
    "\n",
    "    data = [group1, group2, group3, group4, group5, group6, group7, \\\n",
    "            group8, group9, group10]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Team-Draft Interleaving and Probabilistic Interleaving\n",
    "\n",
    "Implement Team-Draft and Probabilistic Interleaving, with methods that interleave two rankings, and given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interleaved:\n",
    "    def __init__(self, pair):\n",
    "        \"\"\"\n",
    "        A class containing a pair of rankings, has methods to interleave\n",
    "        these rankings into a list as well as counters to keep track of how\n",
    "        often pair.rankings[0] (or E) wins.\n",
    "        input:\n",
    "            pair: A Pair object as defined earlier in the notebook\n",
    "        \"\"\"\n",
    "        self.pair = pair\n",
    "        self.list = None\n",
    "        self.wins_team_draft = 0\n",
    "        self.wins_probabilistic = 0\n",
    "\n",
    "    def team_draft(self):\n",
    "        \"\"\"\n",
    "        Modifies self.list in place.\n",
    "        Takes the rankings in self.pair and merges them using team draft\n",
    "        interleaving. The interleaved list contains tuples of form\n",
    "        (relevance, ranker).\n",
    "        \"\"\"\n",
    "        interleaved_list = []\n",
    "        available_E = set([i[\"id\"] for i in self.pair.rankings[0].ranking])\n",
    "        available_P = set([i[\"id\"] for i in self.pair.rankings[1].ranking])\n",
    "        available = available_E.union(available_P)\n",
    "        team = [0,0]\n",
    "\n",
    "        while len(available_E.intersection(available)) > 0 \\\n",
    "                and len(available_P.intersection(available)) > 0:\n",
    "\n",
    "            # Flip a coin to determine which ranker is first\n",
    "            ranker = int(team[0] > team[1] or (team[0] == team[1]\n",
    "                         and random.choice([0,1]) == 1))\n",
    "\n",
    "            for document in self.pair.rankings[ranker].ranking:\n",
    "                if document[\"id\"] in available:\n",
    "                    interleaved_list.append((document[\"relevance\"], ranker))\n",
    "                    available.remove(document[\"id\"])\n",
    "                    team[ranker] += 1\n",
    "                    break\n",
    "\n",
    "\n",
    "        self.list = interleaved_list[:3]\n",
    "\n",
    "    def probabilistic(self):\n",
    "        \"\"\"\n",
    "        Modifies self.list in place.\n",
    "        Takes the rankings in self.pair and merges them using probabilistic\n",
    "        interleaving. The interleaved list contains tuples of form\n",
    "        (relevance, ranker).\n",
    "        \"\"\"\n",
    "        l1 = copy.copy(self.pair.rankings[0].ranking)\n",
    "        l2 = copy.copy(self.pair.rankings[1].ranking)\n",
    "        lists = [l1, l2]\n",
    "        interleaved_list = []\n",
    "\n",
    "        # As long as some thing is still true:\n",
    "        while len(l1) > 0 and len(l2) > 0:\n",
    "\n",
    "            # Randomly select one of the lists\n",
    "            ranker = random.choice([0,1])\n",
    "            random_l = lists[ranker]\n",
    "\n",
    "            # Sample d from lx using a softmax\n",
    "            document_rank = self.sample_softmax(len(random_l))\n",
    "            document = random_l[document_rank]\n",
    "\n",
    "            # Put d in l and remove from l1 and l2\n",
    "            interleaved_list.append((document[\"relevance\"], ranker))\n",
    "            for l in lists:\n",
    "                try:\n",
    "                    l.remove(document)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "        self.list = interleaved_list[:3]\n",
    "\n",
    "    def sample_softmax(self, length):\n",
    "        \"\"\"\n",
    "        Returns an integer from 0 to length according to a softmax\n",
    "        function.\n",
    "        \"\"\"\n",
    "        normalization = sum([1/i**3 for i in range(1, length + 1)])\n",
    "\n",
    "        sample = random.random()\n",
    "        total = 0\n",
    "\n",
    "        for i in range(1, length + 1):\n",
    "            total += (1/i**3)/normalization\n",
    "            if sample < total:\n",
    "                return i - 1\n",
    "\n",
    "        # You should never get here error\n",
    "        raise notImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4:  Simulate user clicks\n",
    "Having interleaved all the ranking pairs in each group (for each measure) an online experiment could be ran. However, given that we do not have any real users (and the entire homework is a big simulation) we will simulate user clicks.\n",
    "\n",
    "We consider the Position Based Model (PBM) for clicks. The parameters of PBM are estimated based on the Expectation-Maximization (EM) method. We first learn the parameters of the model given the Yandex Click Log. We predict the click probability given a ranked list of relevance labels. Then, we decide - stochastically - whether a document is clicked based on these probabilities.\n",
    "\n",
    "After training PBM, we use the learnt parameters γr , while instead of the 𝑎uq learnt we use epsilon for the non-relevant documents (for a small value of ) and 1-epsilon for the relevant documents.\n",
    "\n",
    "We further consider and implement a Random Click Model, which will be used for sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "filename = \"../data/YandexRelPredChallenge.txt\"\n",
    "\n",
    "def load_data(data):\n",
    "    '''This function loads data into an appropriate form for further analysis.\n",
    "    Input: Yandex Click Log file\n",
    "    Output: a pandas dataframe with rows of format [QueryID, [ResultIDs], [Clicked]]'''\n",
    "\n",
    "    completed_queries = []\n",
    "    rank = 3\n",
    "\n",
    "    with open(data) as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "\n",
    "            if row[2] == 'Q':\n",
    "                try:\n",
    "                    completed_queries.append(query)\n",
    "                except:\n",
    "                    pass\n",
    "                query = row\n",
    "                query.append([])\n",
    "                query.append([])\n",
    "\n",
    "                for i in range(5, rank+5):\n",
    "                    query[15].append(query[i])\n",
    "                    query[16].append(False)\n",
    "\n",
    "            else:\n",
    "                for i in range(0, rank):\n",
    "                    if row[3] == query[15][i]:\n",
    "                        query[16][i] = True\n",
    "\n",
    "    headers = ['SessionID', 'TimePassed', 'TypeOfAction', 'QueryID', 'RegionID', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 'ResultIDs', 'Clicked']\n",
    "    qd_pairs = pd.DataFrame(completed_queries, columns=headers)\n",
    "    qd_pairs = qd_pairs[['QueryID', 'ResultIDs', 'Clicked']]\n",
    "\n",
    "    return qd_pairs\n",
    "\n",
    "def learn_model_parameters(qd_pairs):\n",
    "\n",
    "    '''This method takes as input the Yandex click log. It trains the alpha and\n",
    "     gamma parameters for the PBM model on the basis of this file. It saves the\n",
    "    alphas and gammas (so they only have to be trained once) to a csv and\n",
    "    returns them.'''\n",
    "\n",
    "    iterations = 50\n",
    "\n",
    "    ## EM algorithm for finding optimal alpha and gamma ##\n",
    "\n",
    "    # Set parameters to initial values\n",
    "    max_rank = 3\n",
    "    alphas = defaultdict(lambda: 0.5)\n",
    "    gammas = [0.5] * max_rank\n",
    "\n",
    "    for ctr in range(0, iterations):\n",
    "        print(\"Round \" + str(ctr))\n",
    "\n",
    "        print(\"Update alphas\")\n",
    "\n",
    "        # update one alpha for each QD pair\n",
    "        new_alphas = defaultdict(lambda:1)\n",
    "        qd_count = defaultdict(lambda:2)\n",
    "\n",
    "        for i, session in qd_pairs.iterrows():\n",
    "\n",
    "            if i%10000 == 0:\n",
    "                print(str(i) + \" sessions visited\")\n",
    "\n",
    "            for rank in range(max_rank):\n",
    "                query = session[\"QueryID\"]\n",
    "                result = session[\"ResultIDs\"][rank]\n",
    "                click_u = float(session[\"Clicked\"][rank])\n",
    "\n",
    "                old_alpha = max(alphas[(query, result)], 0.000001)\n",
    "                old_gamma = max(gammas[rank], 0.000001)\n",
    "\n",
    "                qd_count[(query, result)] += 1\n",
    "\n",
    "                # add new weights to dictionary\n",
    "                new_alphas[(query, result)] += click_u + (1-click_u) * \\\n",
    "                ((1-old_gamma)*old_alpha / (1-old_gamma*old_alpha))\n",
    "\n",
    "        for key, value in qd_count.items():\n",
    "            new_alphas[key] /= value\n",
    "\n",
    "        alphas = new_alphas\n",
    "\n",
    "\n",
    "        print(\"Update gammas\")\n",
    "\n",
    "        new_gammas = [0] * max_rank\n",
    "\n",
    "        # update one gamma per rank\n",
    "        for i, session in qd_pairs.iterrows():\n",
    "\n",
    "            if i%10000 == 0:\n",
    "                print(str(i) + \" sessions visited\")\n",
    "\n",
    "            for rank in range(max_rank):\n",
    "                query = session[\"QueryID\"]\n",
    "                result = session[\"ResultIDs\"][rank]\n",
    "                click_u = float(session[\"Clicked\"][rank])\n",
    "                old_gamma = gammas[rank]\n",
    "                old_alpha = alphas[(query, result)]\n",
    "\n",
    "                new_gammas[rank] += click_u + (1-click_u) * \\\n",
    "                    (old_gamma*(1-old_alpha)) / (1-old_gamma*old_alpha)\n",
    "\n",
    "        for rank, value in enumerate(gammas):\n",
    "            gammas[rank] = new_gammas[rank] / qd_pairs.shape[0]\n",
    "\n",
    "        print(gammas)\n",
    "\n",
    "    alphas_df = pd.DataFrame.from_dict(alphas, orient='index')\n",
    "    alphas_df.to_csv('trained_alphas.csv')\n",
    "\n",
    "    gammas_df = pd.DataFrame({'gammas': gammas})\n",
    "    gammas_df.to_csv('trained_gammas.csv')\n",
    "\n",
    "    return alphas, gammas\n",
    "\n",
    "def get_gammas_from_file():\n",
    "    '''Use this method to load gammas when parameters are already trained.'''\n",
    "\n",
    "    try:\n",
    "        gammas = pd.read_csv('trained_gammas.csv')\n",
    "        gammas = gammas['gammas'].tolist()\n",
    "\n",
    "        return gammas\n",
    "\n",
    "    except:\n",
    "        print(\"File not found!\")\n",
    "\n",
    "def predict_click_probability(interleaving, gammas):\n",
    "\n",
    "    '''This method takes as input an Interleaving object and a list\n",
    "    of gamma parameters that determine the examination probability per rank.\n",
    "    The method calculates its own alpha parameters. It returns the click\n",
    "    probabilities of the ranked list (also as a list).'''\n",
    "\n",
    "    # set epsilon to small value (prob that a not-relevant document is clicked)\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    click_probabilities = []\n",
    "\n",
    "    for rank, item in enumerate(interleaving.list):\n",
    "\n",
    "        relevance = item[0]\n",
    "\n",
    "        # check relevance label to determine alphas\n",
    "        if relevance == 1:\n",
    "            alpha = 1-epsilon\n",
    "        else:\n",
    "            alpha = epsilon\n",
    "\n",
    "        # determine gamma\n",
    "        gamma = gammas[rank]\n",
    "\n",
    "        click_prob = alpha * gamma\n",
    "        click_probabilities.append(click_prob)\n",
    "\n",
    "    return click_probabilities\n",
    "\n",
    "\n",
    "\n",
    "def click_documents(interleaved, click_probabilities):\n",
    "    '''This method takes as input a ranked list of documents (Interleaved.list)\n",
    "    and the click probabilities for each rank. It returns an integer representing\n",
    "    whether Interleaved.pair[0] (E) won or not.'''\n",
    "\n",
    "    clicked = Counter()\n",
    "\n",
    "    for rank, item in enumerate(interleaved.list):\n",
    "\n",
    "        rand = random.uniform(0, 1)\n",
    "\n",
    "        if rand < click_probabilities[rank]:\n",
    "            clicked[item[1]] += 1\n",
    "\n",
    "    # If it's a tie return 0.5, else return 1 if E wins and 0 if P wins.\n",
    "    if clicked[0] == clicked[1]:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return int(clicked[0] > clicked[1])\n",
    "\n",
    "def random_click_model(qd_pairs):\n",
    "\n",
    "    '''This method takes the dataset with all sessions and returns the click\n",
    "    probabilities trained using the random click model. Probabilities are returned\n",
    "    in a length of maximum rank for convenience.'''\n",
    "    max_rank = 3\n",
    "    clicks = 0\n",
    "    docs = 0\n",
    "\n",
    "    for i, session in qd_pairs.iterrows():\n",
    "        for rank in range(max_rank):\n",
    "            docs += 1\n",
    "            if session[\"Clicked\"][rank]:\n",
    "                clicks += 1\n",
    "\n",
    "    click_prob = clicks / docs\n",
    "\n",
    "    click_probs = [click_prob] * max_rank\n",
    "\n",
    "    return click_probs\n",
    "\n",
    "def get_model_parameters():\n",
    "    \"\"\"\n",
    "    Tries to take gammas from a file, if the file doesn't exist train\n",
    "    parameters to get the gammas.\n",
    "    \"\"\"\n",
    "    gammas = get_gammas_from_file()\n",
    "    if not gammas == None:\n",
    "        return gammas\n",
    "    else:\n",
    "        qd_pairs = load_data(data)\n",
    "        _, gammas = learn_model_parameters(qd_pairs)\n",
    "        return gammas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9956892329152498, 0.4826360728234162, 0.3336191638862573]\n"
     ]
    }
   ],
   "source": [
    "data = \"data/YandexRelPredChallenge.txt\"\n",
    "\n",
    "gammas = get_model_parameters()\n",
    "print(gammas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Simulate interleaving experiment\n",
    "Having implemented the click model, it is time to run the simulated experiment.\n",
    "\n",
    "For each of interleaving experiment run k simulations for each one of the two click models implemented and measure the proportion p of wins for E. Group these proportions in the respective group the interleaved ranking came from. The larger the k the better, but also the larger the k the longer it takes to run the experiment; so make a reasonable choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_experiment(interleaving, gammas, k):\n",
    "    \"\"\"\n",
    "    Modifies an Interleaved object in place to contain the number of wins \n",
    "    for both interleaving methods.\n",
    "    \"\"\"\n",
    "    cntr = Counter()\n",
    "    for i in range(k):\n",
    "        # Run the experiment for team draft interleaving\n",
    "        interleaving.team_draft()\n",
    "        win = click_documents(interleaving, predict_click_probability(interleaving, gammas))\n",
    "        interleaving.wins_team_draft += win/k\n",
    "        cntr[win] += 1\n",
    "\n",
    "        # Run the experiment for probabilistic interleaving\n",
    "        interleaving.probabilistic()\n",
    "        win = click_documents(interleaving, predict_click_probability(interleaving, gammas))\n",
    "        interleaving.wins_probabilistic += win/k\n",
    "        cntr[win] += 1\n",
    "\n",
    "def simulate_experiment(pairs, gammas, k):\n",
    "    \"\"\"\n",
    "    Creates a list of Interleaved objects that contain both ERR and number\n",
    "    of wins for the team draft and probabilistic methods of interleaving.\n",
    "    \"\"\"\n",
    "    interleavings = []\n",
    "    for i, pair in enumerate(pairs):\n",
    "        interleaving = Interleaved(pair)\n",
    "\n",
    "        # Modify the interleaving object in place\n",
    "        online_experiment(interleaving, gammas, k)\n",
    "\n",
    "        interleavings.append(interleaving)\n",
    "\n",
    "    return interleavings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the experiment results (only takes ~30 sec)\n",
    "interleavings = simulate_experiment(pairs, gammas, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, interleaving in enumerate(interleavings):\n",
    "    plt.plot(interleaving.pair.delta_ERR, interleaving.wins_team_draft, 'ro')\n",
    "    plt.plot(interleaving.pair.delta_ERR, interleaving.wins_probabilistic, 'bo')\n",
    "    \n",
    "plt.xlabel(\"ERR\")\n",
    "plt.ylabel(\"n_wins E\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Compute sample size\n",
    "Use each one of the afore-computed proportions to compute the sample size needed to detect such a proportion in a statistically significant manner. Allow a chance of falsely rejecting the null hypothesis (i.e. concluding that E is better than P, when it is not) of 5% and a chance of falsely not rejecting the null hypothesis (i.e. not concluding that E is better than P, when it is) of 10%. Use the values above for a power analysis of the proportion test, for the 1-sided case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results team-draft interleaving:\n",
      "+------------+---------+---------+-----------+\n",
      "| Delta_ERR  | Minimum | Median  |  Maximum  |\n",
      "+============+=========+=========+===========+\n",
      "| [0.05-0.1) | 94.563  | 550.294 | 1.177e+31 |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.1-0.2)  | 17.896  | 241.081 | 1.177e+31 |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.2-0.3)  | 13.252  | 48.762  | 1.177e+31 |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.3-0.4)  | 8.301   | 14.045  | 219.005   |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.4-0.5)  | 8.159   | 9.986   | 16.854    |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.5-0.6)  | 8.206   | 9.481   | 10.265    |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.6-0.7)  | 8.206   | 8.301   | 8.621     |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.7-0.8)  | 0       | 0       | 0         |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.8-0.9)  | 0       | 0       | 0         |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.9-0.95] | 0       | 0       | 0         |\n",
      "+------------+---------+---------+-----------+\n",
      "\n",
      "\n",
      "Results probabilistic interleaving:\n",
      "+------------+---------+---------+-----------+\n",
      "| Delta_ERR  | Minimum | Median  |  Maximum  |\n",
      "+============+=========+=========+===========+\n",
      "| [0.05-0.1) | 127.453 | 726.671 | 1.177e+31 |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.1-0.2)  | 50.233  | 261.123 | 2868.949  |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.2-0.3)  | 34.963  | 55.062  | 129.902   |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.3-0.4)  | 20.429  | 31.826  | 73.985    |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.4-0.5)  | 16.917  | 19.790  | 27.028    |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.5-0.6)  | 13.387  | 15.117  | 17.430    |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.6-0.7)  | 11.245  | 11.878  | 12.900    |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.7-0.8)  | 0       | 0       | 0         |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.8-0.9)  | 0       | 0       | 0         |\n",
      "+------------+---------+---------+-----------+\n",
      "| [0.9-0.95] | 0       | 0       | 0         |\n",
      "+------------+---------+---------+-----------+\n"
     ]
    }
   ],
   "source": [
    "def determine_sample_size(alpha, beta, p1):\n",
    "    \n",
    "    p0 = 0.5\n",
    "    z_alpha = st.norm.ppf(1-alpha) \n",
    "    z_beta = st.norm.ppf(1-beta)\n",
    "    min_N = ((z_alpha * (p0*(1-p0))**.5 + z_beta * (p1*(1-p1))) / (np.abs(p1-p0)))**2\n",
    "    \n",
    "    # continutity correction \n",
    "    N = min_N + 1 / (np.abs(p1-p0))\n",
    "    \n",
    "    return N\n",
    "\n",
    "def sample_size_interval(all_sample_sizes):\n",
    "    if not all_sample_sizes:\n",
    "        minimum, median, maximum = 0, 0, 0\n",
    "        \n",
    "    else: \n",
    "        np.array(all_sample_sizes)\n",
    "        median = np.median(all_sample_sizes)\n",
    "        maximum = np.max(all_sample_sizes)\n",
    "        minimum = np.min(all_sample_sizes)\n",
    "    \n",
    "    return minimum, median, maximum\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.1\n",
    "all_n_team_draft = []\n",
    "all_n_probabilistic = []\n",
    "\n",
    "for interleaving in interleavings:\n",
    "    p1_t = interleaving.wins_team_draft\n",
    "    p1_p = interleaving.wins_probabilistic\n",
    "    \n",
    "    delta_ERR_t = interleaving.pair.delta_ERR\n",
    "    delta_ERR_p = interleaving.pair.delta_ERR\n",
    "    \n",
    "    N_t = determine_sample_size(alpha, beta, p1_t)\n",
    "    N_p = determine_sample_size(alpha, beta, p1_p)\n",
    "    all_n_team_draft.append((delta_ERR_t, N_t))\n",
    "    all_n_probabilistic.append((delta_ERR_p, N_p))\n",
    "    \n",
    "# seperate results based on delta_ERR\n",
    "group1 = [result[1] for result in all_n_team_draft if (result[0]>=0.05 and result[0]<0.1)]\n",
    "group2 = [result[1] for result in all_n_team_draft if (result[0]>=0.1 and result[0]<0.2)]\n",
    "group3 = [result[1] for result in all_n_team_draft if (result[0]>=0.2 and result[0]<0.3)]\n",
    "group4 = [result[1] for result in all_n_team_draft if (result[0]>=0.3 and result[0]<0.4)]\n",
    "group5 = [result[1] for result in all_n_team_draft if (result[0]>=0.4 and result[0]<0.5)]\n",
    "group6 = [result[1] for result in all_n_team_draft if (result[0]>=0.5 and result[0]<0.6)]\n",
    "group7 = [result[1] for result in all_n_team_draft if (result[0]>=0.6 and result[0]<0.7)]\n",
    "group8 = [result[1] for result in all_n_team_draft if (result[0]>=0.7 and result[0]<0.8)]\n",
    "group9 = [result[1] for result in all_n_team_draft if (result[0]>=0.8 and result[0]<0.9)]\n",
    "group10 = [result[1] for result in all_n_team_draft if (result[0]>=0.9 and result[0]<=0.95)]\n",
    "\n",
    "results_team_draft = {'[0.05-0.1)': group1, '[0.1-0.2)': group2, '[0.2-0.3)': group3, '[0.3-0.4)': group4, \\\n",
    "                      '[0.4-0.5)': group5, '[0.5-0.6)': group6, '[0.6-0.7)': group7, '[0.7-0.8)': group8, \\\n",
    "                      '[0.8-0.9)': group9, '[0.9-0.95]': group10}\n",
    "\n",
    "group1_p = [result[1] for result in all_n_probabilistic if (result[0]>=0.05 and result[0]<0.1)]\n",
    "group2_p = [result[1] for result in all_n_probabilistic if (result[0]>=0.1 and result[0]<0.2)]\n",
    "group3_p = [result[1] for result in all_n_probabilistic if (result[0]>=0.2 and result[0]<0.3)]\n",
    "group4_p = [result[1] for result in all_n_probabilistic if (result[0]>=0.3 and result[0]<0.4)]\n",
    "group5_p = [result[1] for result in all_n_probabilistic if (result[0]>=0.4 and result[0]<0.5)]\n",
    "group6_p = [result[1] for result in all_n_probabilistic if (result[0]>=0.5 and result[0]<0.6)]\n",
    "group7_p = [result[1] for result in all_n_probabilistic if (result[0]>=0.6 and result[0]<0.7)]\n",
    "group8_p = [result[1] for result in all_n_probabilistic if (result[0]>=0.7 and result[0]<0.8)]\n",
    "group9_p = [result[1] for result in all_n_probabilistic if (result[0]>=0.8 and result[0]<0.9)]\n",
    "group10_p = [result[1] for result in all_n_probabilistic if (result[0]>=0.9 and result[0]<=0.95)]\n",
    "\n",
    "results_probabilistic = {'[0.05-0.1)': group1_p, '[0.1-0.2)': group2_p, '[0.2-0.3)': group3_p, '[0.3-0.4)': group4_p, \\\n",
    "                         '[0.4-0.5)': group5_p, '[0.5-0.6)': group6_p, '[0.6-0.7)': group7_p, '[0.7-0.8)': group8_p, \\\n",
    "                         '[0.8-0.9)': group9_p, '[0.9-0.95]': group10_p}\n",
    "\n",
    "minimums_t, minimums_p, medians_t, medians_p, maximums_t, maximums_p = [], [], [], [], [], []\n",
    "\n",
    "for group in results_team_draft.values():\n",
    "    minimum_t, median_t, maximum_t = sample_size_interval(group)\n",
    "    minimums_t.append(minimum_t)\n",
    "    medians_t.append(median_t)\n",
    "    maximums_t.append(maximum_t)\n",
    "    \n",
    "for group in results_probabilistic.values():\n",
    "    minimum_p, median_p, maximum_p = sample_size_interval(group)\n",
    "    minimums_p.append(minimum_p)\n",
    "    medians_p.append(median_p)\n",
    "    maximums_p.append(maximum_p)\n",
    "    \n",
    "  \n",
    "print(\"Results team-draft interleaving:\")    \n",
    "table_t = tt.Texttable()\n",
    "headers = ['Delta_ERR', 'Minimum', 'Median', 'Maximum']\n",
    "table_t.header(headers)\n",
    "deltaERR = ['[0.05-0.1)', '[0.1-0.2)', '[0.2-0.3)', '[0.3-0.4)', '[0.4-0.5)', '[0.5-0.6)', '[0.6-0.7)', \\\n",
    "            '[0.7-0.8)', '[0.8-0.9)', '[0.9-0.95]']\n",
    "\n",
    "for row in zip(deltaERR, minimums_t, medians_t, maximums_t):\n",
    "    table_t.add_row(row)\n",
    "tab_t = table_t.draw()\n",
    "print(tab_t)\n",
    "print('\\n')\n",
    "print(\"Results probabilistic interleaving:\")    \n",
    "table_p = tt.Texttable()\n",
    "table_p.header(headers)\n",
    "\n",
    "for row in zip(deltaERR, minimums_p, medians_p, maximums_p):\n",
    "    table_p.add_row(row)\n",
    "tab_p = table_p.draw()\n",
    "print(tab_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Analysis\n",
    "\n",
    "* Report the aforementioned tables for the Random Click Model and PBM and for the two interleaving methods [5pts];\n",
    "* Analyze the results and report your conclusions by observing the results of the experiment [10pts];\n",
    "* Based on the literature, suggest possible improvements to the experimental design and how would you implement them [5pts].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we can see in both tables that the required sample size decreases when the difference between algorithm E and P becomes larger. This makes sense, since we assume that a more extreme outcome in the offline experiment will need less impressions to get a significant result in the online experiment. Moreover, we see a large difference in the range of the sample sizes. If the proportions $p_0$ and $p_1$ are close to each other, the number of required impressions becomes very large. Hence, the maximum value goes to infinity. \n",
    "\n",
    "Secondly, it is evident that the team-draft interleaving algorithm needs less samples to get significant results. However, the variance is higher than in probabilistic interleaving, as the maximum values are more rapidly decreasing for this algorithm. It can be concluded that choosing the top result (as is done for team-draft interleaving) is not always beneficial. On the contrary, uncertainty in the sample size can actually be decreased by allowing some probabilistic distribution over choosing a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
