{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework part B: implementing a sample size estimator for interleaving experiments\n",
    "\n",
    "#### by Kim de Bie, Bram van den Heuvel and Kiki van Rongen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simulate Rankings of Relevance for E and P\n",
    "\n",
    "In the first step generate pairs of rankings, for the production P and experimental E, respectively. Assume a binary relevance. Make no assumption regarding the documents returned by the two algorithms (they can be distinct but they may also overlap). Further, assume that the algorithms are used on mobiles, so we are interested only in rankings of length 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranking:\n",
    "\n",
    "    def __init__(self, id, ranker):\n",
    "        self.id = id\n",
    "        self.ERR = 0\n",
    "        self.ranker = ranker\n",
    "        self.ranking = [{'id': 1, 'relevance': 0}, {'id': 2, 'relevance': 0}, {'id': 3, 'relevance': 0}]\n",
    "\n",
    "    def set_ERR(self):\n",
    "        self.ERR = calculate_ERR(self.ranking)\n",
    "\n",
    "class Pair:\n",
    "    def __init__(self, ranking_E, ranking_P):\n",
    "        self.rankings = [ranking_E, ranking_P]\n",
    "        self.delta_ERR = ranking_E.ERR - ranking_P.ERR\n",
    "\n",
    "def calculate_ERR(ranking):\n",
    "\n",
    "    rl1, rl2, max = 0, 0, 0\n",
    "\n",
    "    # find maximum of relevance labels\n",
    "    for idx, docu in enumerate(ranking):\n",
    "        rl = docu['relevance']\n",
    "\n",
    "        # store relevance label of first and second document\n",
    "        if idx == 0:\n",
    "            rl1 = rl\n",
    "        elif idx == 1:\n",
    "            rl2 = rl\n",
    "\n",
    "        # update maximum\n",
    "        if rl > max:\n",
    "            max = rl\n",
    "\n",
    "    # calculate probability of stopping at first and second document\n",
    "    prob1 = (2**rl1-1)/(2**max)\n",
    "    prob2 = (2**rl2-1)/(2**max)\n",
    "\n",
    "    # calculate ERR as sum of products\n",
    "    ERR = 0\n",
    "    for i in range(2,4):\n",
    "\n",
    "        if i == 2:\n",
    "            prod2 = (1 - prob1) * prob1 * (1 / i)\n",
    "            ERR += prod2\n",
    "\n",
    "        elif i == 3:\n",
    "            prod3 = (1 - prob1) * prob1 * (1 - prob2) * prob2 * (1 / i)\n",
    "            ERR += prod3\n",
    "\n",
    "    return ERR\n",
    "\n",
    "def create_pairs():\n",
    "\n",
    "    \"\"\"\n",
    "    This function creates two ranked lists of documents for algorithm P and E.\n",
    "    Subsequently, it forms E-P pairs of possible rankings.\n",
    "    \"\"\"\n",
    "\n",
    "    ############### CREATE RANKED LIST FOR E ###############\n",
    "\n",
    "    rankings_E = []\n",
    "    rankings_P = []\n",
    "\n",
    "    # define all possible combinations of relevance labels\n",
    "    rl_permutations = [[0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [1, 0, 1], [0, 1, 1], [1, 1, 1]]\n",
    "\n",
    "    # assign id to the rankings\n",
    "    id = 0\n",
    "\n",
    "    # iterate over permutations of relevance labels\n",
    "    for rl_perm in rl_permutations:\n",
    "\n",
    "        # create ranking object and adjust its relevance labels\n",
    "        rank_E = Ranking(id, 'E')\n",
    "        for i, document_E in enumerate(rank_E.ranking):\n",
    "             document_E['relevance'] = rl_perm[i]\n",
    "\n",
    "        # calculate ERR\n",
    "        rank_E.set_ERR()\n",
    "\n",
    "        # store ranking list for E\n",
    "        rankings_E.append(rank_E)\n",
    "        id += 1\n",
    "\n",
    "    ############### CREATE RANKED LIST FOR P ###############\n",
    "\n",
    "    # define all possible id's for P\n",
    "    id_permutations = [[1, 5, 6], [2, 5, 6], [3, 5, 6], [4, 5, 6], \\\n",
    "                       [4, 1, 6], [4, 2, 6], [4, 3, 6], \\\n",
    "                       [4, 5, 1], [4, 5, 2], [4, 5, 3], \\\n",
    "                       [1, 2, 6], [1, 5, 2], [4, 1, 2], \\\n",
    "                       [1, 3, 6], [1, 5, 3], [4, 1, 3], \\\n",
    "                       [2, 3, 6], [2, 5, 3], [4, 2, 3], \\\n",
    "                       [2, 1, 6], [2, 5, 1], [4, 2, 1], \\\n",
    "                       [3, 1, 6], [3, 5, 1], [4, 3, 1], \\\n",
    "                       [3, 2, 6], [3, 5, 2], [4, 3, 2], \\\n",
    "                       [1, 2, 3], [1, 3, 2], [3, 1, 2], [3, 2, 1], [2, 1, 3], [2, 3, 1]]\n",
    "\n",
    "    # iterate over possible id's for P\n",
    "    for id_perm in id_permutations:\n",
    "\n",
    "        # iterate over permutations of relevance labels\n",
    "        for rl_perm in rl_permutations:\n",
    "\n",
    "            # create ranking object for P\n",
    "            rank_P = Ranking(id, 'P')\n",
    "\n",
    "            # adjust relevance labels & id numbers\n",
    "            for j, document_P in enumerate(rank_P.ranking):\n",
    "                document_P['relevance'] = rl_perm[j]\n",
    "                document_P['id'] = id_perm[j]\n",
    "\n",
    "            # calculate ERR\n",
    "            rank_P.set_ERR()\n",
    "\n",
    "            # store ranking list for P\n",
    "            rankings_P.append(rank_P)\n",
    "            id += 1\n",
    "\n",
    "    ############### FORM E-P PAIRS ###############\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for rank_E in rankings_E:\n",
    "\n",
    "        # store ids and relevance labels of E\n",
    "        ids_E = [1, 2, 3]\n",
    "        rl_E = [d.get('relevance') for d in rank_E.ranking]\n",
    "\n",
    "        for rank_P in rankings_P:\n",
    "\n",
    "            # keep track of errors (duplicates with non-matching rl's)\n",
    "            error = False\n",
    "\n",
    "            # store ids and relevance labels of P\n",
    "            ids_P = [d.get('id') for d in rank_P.ranking]\n",
    "            rl_P = [d.get('relevance') for d in rank_P.ranking]\n",
    "\n",
    "            # iterate over ids of P\n",
    "            for idx, id in enumerate(ids_P):\n",
    "\n",
    "                # check for duplicates with non-matching relevance labels\n",
    "                if (id in ids_E) & (rl_P[idx] != rl_E[idx]):\n",
    "                    error = True\n",
    "                    break\n",
    "\n",
    "            # create pair and add to list, if no error occurs\n",
    "            if not error:\n",
    "                pair = Pair(rank_E, rank_P)\n",
    "                pairs.append(pair)\n",
    "\n",
    "    return pairs\n",
    "\n",
    "    \n",
    "pairs = create_pairs()\n",
    "# remove pairs with deltaERR lower than 0.05\n",
    "data = [p for p in pairs if p.delta_ERR>=0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate the 𝛥ERR\n",
    "Implement the aforementioned measure, ERR.\n",
    "\n",
    "For all P and E ranking pairs constructed above calculate the difference: 𝛥measure = measureE-measureP. Consider only those pairs for which E outperforms P and group them such that group 1 contains all pairs for which 0.05 < 𝛥measure ≤ 0.1, group 2 all pairs for which 0.1 < 𝛥measure ≤ 0.2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See previous step for loading pairs and calculation of (delta-)ERR.\n",
    "\n",
    "# TODO: group pairs in containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Team-Draft Interleaving and Probabilistic Interleaving\n",
    "\n",
    "Implement Team-Draft and Probabilistic Interleaving, with methods that interleave two rankings, and given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interleaved:\n",
    "    def __init__(self, pair):\n",
    "        \"\"\"\n",
    "        blablabla\n",
    "        \"\"\"\n",
    "        self.pair = pair\n",
    "        self.list = None\n",
    "\n",
    "    def team_draft(self):\n",
    "        \"\"\"\n",
    "        Takes the rankings in pair and merges them using team draft\n",
    "        interleaving. The interleaved list contains tuples of form\n",
    "        (relevance, ranker).\n",
    "        \"\"\"\n",
    "        interleaved_list = []\n",
    "        available_E = set([i[\"id\"] for i in self.pair.rankings[0].ranking])\n",
    "        available_P = set([i[\"id\"] for i in self.pair.rankings[1].ranking])\n",
    "        available = available_E.union(available_P)\n",
    "        team = [0,0]\n",
    "\n",
    "        while len(available_E.intersection(available)) > 0 \\\n",
    "                and len(available_P.intersection(available)) > 0:\n",
    "\n",
    "            # Flip a coin to determine which ranker is first\n",
    "            ranker = int(team[0] > team[1] or (team[0] == team[1]\n",
    "                         and random.choice([0,1]) == 1))\n",
    "\n",
    "            for document in self.pair.rankings[ranker].ranking:\n",
    "                if document[\"id\"] in available:\n",
    "                    interleaved_list.append((document[\"relevance\"], ranker))\n",
    "                    available.remove(document[\"id\"])\n",
    "                    team[ranker] += 1\n",
    "                    break\n",
    "\n",
    "\n",
    "        self.list = interleaved_list\n",
    "\n",
    "    def probabilistic(self):\n",
    "        # Get lists l1 and l2 and interleaved list l\n",
    "        l1 = copy.copy(self.pair.rankings[0].ranking)\n",
    "        l2 = copy.copy(self.pair.rankings[1].ranking)\n",
    "        lists = [l1, l2]\n",
    "        interleaved_list = []\n",
    "\n",
    "        # As long as some thing is still true: TODO: This may be wrong\n",
    "        while len(l1) > 0 and len(l2) > 0:\n",
    "\n",
    "            # Randomly select one of the lists\n",
    "            ranker = random.choice([0,1])\n",
    "            random_l = lists[ranker]\n",
    "\n",
    "            # Sample d from lx using a softmax\n",
    "            document_rank = self.sample_softmax(len(random_l))\n",
    "            document = random_l[document_rank]\n",
    "\n",
    "            # Put d in l and remove from l1 and l2\n",
    "            interleaved_list.append((document[\"relevance\"], ranker))\n",
    "            for l in lists:\n",
    "                try:\n",
    "                    l.remove(document)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "        self.list = interleaved_list\n",
    "\n",
    "    def sample_softmax(self, length):\n",
    "        \"\"\"\n",
    "        Returns an integer from 0 to length according to a softmax\n",
    "        function.\n",
    "        \"\"\"\n",
    "        normalization = sum([1/i**3 for i in range(1, length + 1)])\n",
    "\n",
    "        sample = random.random()\n",
    "        total = 0\n",
    "\n",
    "        for i in range(1, length + 1):\n",
    "            total += (1/i**3)/normalization\n",
    "            if sample < total:\n",
    "                return i - 1\n",
    "\n",
    "        # You should never get here error\n",
    "        raise notImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4:  Simulate user clicks\n",
    "Having interleaved all the ranking pairs in each group (for each measure) an online experiment could be ran. However, given that we do not have any real users (and the entire homework is a big simulation) we will simulate user clicks.\n",
    "\n",
    "Consider a click model, namely the Position Based Model (PBM). The parameters of PBM can be estimated based on the Expectation-Maximization (EM) method. Implement PBM so that (a) there is a method that learns the parameters of the model given a set of training data, (b) there is a method that predicts the click probability given a ranked list of relevance labels, (c) there is a method that decides - stochastically - whether a document is clicked based on these probabilities.\n",
    "\n",
    "Having implemented the PBM click models, estimate its parameters the Yandex Click Log [file]. \n",
    "\n",
    "After training PBM, use the learnt parameters γr , while instead of the 𝑎uq learnt usefor the non-relevant documents (for a small value of ) and 1- for the relevant documents.\n",
    "\n",
    "Further consider and implement a Random Click Model, which will be used for sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "Update alphas\n",
      "0 sessions visited\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-722fd60d432c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0malphas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m \u001b[0malphas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgammas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearn_model_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_gammas_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-722fd60d432c>\u001b[0m in \u001b[0;36mlearn_model_parameters\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mqd_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mqd_pairs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36miterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[1;32m--> 264\u001b[1;33m                                        raise_cast_failure=True)\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_sanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[0;32m   3193\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3194\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3195\u001b[1;33m             \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3197\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_try_cast\u001b[1;34m(arr, take_fast_path)\u001b[0m\n\u001b[0;32m   3160\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3161\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_cast_to_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3162\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_extension_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3163\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_extension_type\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m   1644\u001b[0m     \"\"\"\n\u001b[0;32m   1645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1646\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mis_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1647\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1648\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_categorical\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m    218\u001b[0m     \"\"\"\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCCategorical\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\generic.py\u001b[0m in \u001b[0;36m_check\u001b[1;34m(cls, inst)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_typ'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__instancecheck__\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_check\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_check\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = \"data/YandexRelPredChallenge.txt\"\n",
    "\n",
    "def learn_model_parameters(data):\n",
    "\n",
    "    '''This method takes as input the Yandex click log. It first cleans the file\n",
    "    and then trains the alpha and gamma parameters for the PBM model on the basis\n",
    "    of this file. It saves the alphas and gammas (so they only have to be trained\n",
    "    once) to a csv and returns them.'''\n",
    "\n",
    "    # Cleaning data\n",
    "\n",
    "    completed_queries = []\n",
    "\n",
    "    with open(data) as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "\n",
    "            if row[2] == 'Q':\n",
    "                try:\n",
    "                    completed_queries.append(query)\n",
    "                except:\n",
    "                    pass\n",
    "                query = row\n",
    "                query.append([])\n",
    "                query.append([])\n",
    "\n",
    "                for i in range(5, 8):\n",
    "                    query[15].append(query[i])\n",
    "                    query[16].append(False)\n",
    "\n",
    "            else:\n",
    "                for i in range(0, 3):\n",
    "                    if row[3] == query[15][i]:\n",
    "                        query[16][i] = True\n",
    "\n",
    "    headers = ['SessionID', 'TimePassed', 'TypeOfAction', 'QueryID', 'RegionID', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 'ResultIDs', 'Clicked']\n",
    "    qd_pairs = pd.DataFrame(completed_queries, columns=headers)\n",
    "    qd_pairs = qd_pairs[['QueryID', 'ResultIDs', 'Clicked']]\n",
    "\n",
    "    ## EM algorithm for finding optimal alpha and gamma ##\n",
    "\n",
    "    # Set parameters to initial values\n",
    "    max_rank = 3\n",
    "\n",
    "    # alpha (attractiveness) is defined per unique query-document pair\n",
    "    alphas = defaultdict(lambda: 0.5)\n",
    "\n",
    "    # gamma (examination) is defined per rank\n",
    "    gammas = [0.5] * max_rank\n",
    "\n",
    "    for ctr in range(0, 50):\n",
    "        print(\"Round \" + str(ctr))\n",
    "\n",
    "        print(\"Update alphas\")\n",
    "\n",
    "        # update one alpha for each QD pair\n",
    "        new_alphas = defaultdict(lambda:1)\n",
    "        qd_count = defaultdict(lambda:2)\n",
    "\n",
    "        for i, session in qd_pairs.iterrows():\n",
    "\n",
    "            if i%10000 == 0:\n",
    "                print(str(i) + \" sessions visited\")\n",
    "\n",
    "            for rank in range(max_rank):\n",
    "                query = session[\"QueryID\"]\n",
    "                result = session[\"ResultIDs\"][rank]\n",
    "                click_u = float(session[\"Clicked\"][rank])\n",
    "\n",
    "                old_alpha = max(alphas[(query, result)], 0.000001)\n",
    "                old_gamma = max(gammas[rank], 0.000001)\n",
    "\n",
    "                qd_count[(query, result)] += 1\n",
    "\n",
    "                # The Formula\n",
    "                new_alphas[(query, result)] += click_u + (1-click_u) * ((1-old_gamma)*old_alpha / (1-old_gamma*old_alpha))\n",
    "\n",
    "\n",
    "        for key, value in qd_count.items():\n",
    "            new_alphas[key] /= value\n",
    "\n",
    "        alphas = new_alphas\n",
    "\n",
    "        print(\"Update gammas\")\n",
    "\n",
    "        new_gammas = [0] * max_rank\n",
    "\n",
    "        # update one gamma per rank\n",
    "        for i, session in qd_pairs.iterrows():\n",
    "\n",
    "            if i%10000 == 0:\n",
    "                print(str(i) + \" sessions visited\")\n",
    "\n",
    "            for rank in range(max_rank):\n",
    "                query = session[\"QueryID\"]\n",
    "                result = session[\"ResultIDs\"][rank]\n",
    "                click_u = float(session[\"Clicked\"][rank])\n",
    "                old_gamma = gammas[rank]\n",
    "                old_alpha = alphas[(query, result)]\n",
    "\n",
    "                new_gammas[rank] += click_u + (1-click_u) * \\\n",
    "                    (old_gamma*(1-old_alpha)) / (1-old_gamma*old_alpha)\n",
    "\n",
    "        for rank, value in enumerate(gammas):\n",
    "            gammas[rank] = new_gammas[rank] / qd_pairs.shape[0]\n",
    "\n",
    "        print(gammas)\n",
    "\n",
    "    alphas_df = pd.DataFrame.from_dict(alphas, orient='index')\n",
    "    alphas_df.to_csv('data/trained_alphas.csv')\n",
    "\n",
    "    gammas_df = pd.DataFrame({'gammas': gammas})\n",
    "    gammas_df.to_csv('data/trained_gammas.csv')\n",
    "\n",
    "    return alphas, gammas\n",
    "\n",
    "alphas, gammas = learn_model_parameters(data)\n",
    "\n",
    "def get_gammas_from_file():\n",
    "    '''Use this method to load gammas when parameters are already trained.'''\n",
    "\n",
    "    try:\n",
    "        gammas = pd.read_csv('data/trained_gammas.csv')\n",
    "        gammas = gammas['gammas'].tolist()\n",
    "\n",
    "        return gammas\n",
    "\n",
    "    except:\n",
    "        print(\"File not found!\")\n",
    "\n",
    "def predict_click_probability(ranked_list, gammas):\n",
    "\n",
    "    '''This method takes as input a ranked list (Interleaved.list) and a list\n",
    "    of gamma parameters that determine the examination probability per rank.\n",
    "    The method calculates its own alpha parameters. It returns the click\n",
    "    probabilities of the ranked list (also as a list).'''\n",
    "\n",
    "    # set epsilon to small value (prob that a not-relevant document is clicked)\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    click_probabilities = []\n",
    "\n",
    "    for rank, item in enumerate(ranked_list):\n",
    "\n",
    "        relevance = item[0]\n",
    "\n",
    "        # check relevance label to determine alphas\n",
    "        if relevance == 1:\n",
    "            alpha = 1-epsilon\n",
    "        else:\n",
    "            alpha = epsilon\n",
    "\n",
    "        # determine gamma\n",
    "        gamma = gammas[rank]\n",
    "\n",
    "        click_prob = alpha * gamma\n",
    "        click_probabilities.append(click_prob)\n",
    "\n",
    "    return click_probabilities\n",
    "\n",
    "\n",
    "\n",
    "def click_documents(ranked_list, click_probabilities):\n",
    "\n",
    "    '''This method takes as input a ranked list of documents (Interleaved.list)\n",
    "    and the click probabilities for each rank. It returns a list of the same\n",
    "    length with Booleans indicating whether a document was clicked or not.'''\n",
    "\n",
    "    clicked = []\n",
    "\n",
    "    for rank, item in enumerate(ranked_list):\n",
    "\n",
    "        rand = random.uniform(0, 1)\n",
    "\n",
    "        if rand < click_probabilities[rank]:\n",
    "            click = True\n",
    "        else:\n",
    "            click = False\n",
    "\n",
    "        clicked.append(click)\n",
    "\n",
    "    return clicked\n",
    "\n",
    "def random_click_model():\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Simulate interleaving experiment\n",
    "Having implemented the click model, it is time to run the simulated experiment.\n",
    "\n",
    "For each of interleaving experiment run k simulations for each one of the two click models implemented and measure the proportion p of wins for E. Group these proportions in the respective group the interleaved ranking came from. The larger the k the better, but also the larger the k the longer it takes to run the experiment; so make a reasonable choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_experiment():\n",
    "    \n",
    "    for k.... do step 3 and 4\n",
    "    \n",
    "    return proportion_of_E_wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Compute sample size\n",
    "Use each one of the afore-computed proportions to compute the sample size needed to detect such a proportion in a statistically significant manner. Allow a chance of falsely rejecting the null hypothesis (i.e. concluding that E is better than P, when it is not) of 5% and a chance of falsely not rejecting the null hypothesis (i.e. not concluding that E is better than P, when it is) of 10%. Use the values above for a power analysis of the proportion test, for the 1-sided case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_sample_size(results_of_one_experiment):\n",
    "    \n",
    "    all_sample_sizes.append(sample_size)\n",
    "    \n",
    "\n",
    "def sample_size_interval(all_sample_sizes):\n",
    "    \n",
    "    return minimum, median, maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Analysis\n",
    "\n",
    "* Report the aforementioned tables for the Random Click Model and PBM and for the two interleaving methods [5pts];\n",
    "* Analyze the results and report your conclusions by observing the results of the experiment [10pts];\n",
    "* Based on the literature, suggest possible improvements to the experimental design and how would you implement them [5pts].\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
